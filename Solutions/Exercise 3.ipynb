{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.linalg as la\n",
    "from Logistic_funcs import grad_lb, grad2_lb, negloglike"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Line Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem A\n",
    "\n",
    "We have, in our basic steepest descent method, $$\\beta_{k+1} = \\beta_k + \\alpha_k \\nabla l(\\beta_k)$$\n",
    "\n",
    "Thus $p_k = \\nabla l(\\beta_k)$ in this instance.\n",
    "\n",
    "We use the backtracking method to ensure the curvature condition, thus finding the maximum alpha that satisfies the sufficient decrease condition.\n",
    "This prevents us from choosing an alpha that is \"too small\" and must necessarily satisfy the condition without ensuring the progress we want in our algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def linesearch(x, p, f, gradf, c=.5, iters=1000):\n",
    "    '''\n",
    "     This function uses the wolf conditions to compute the \"optimal\" stepsize for \n",
    "     the next step in a descent method.\n",
    "\n",
    "     Inputs:\n",
    "     x: A P vector. The current guess vector for the optimizer of the algorithm\n",
    "     p: A P vector. The descent direction.\n",
    "     f: A function that takes in x as its only non-keyword argument. \n",
    "     gradf: A function that takes in x as its only non-keyword argument. This computes\n",
    "         the gradient of f at x.\n",
    "     c: A float between 0 and 1, this constant value is used in the sufficient decrease\n",
    "         condition\n",
    "     iters: the number of points to try between 0 and 1 for alpha (the stepsize)\n",
    "     \n",
    "     Returns:\n",
    "     suff: A boolean parameter that indicates whether the sufficiency condition was\n",
    "         satisfied\n",
    "     a: The desired stepsize\n",
    "    '''\n",
    "    # Compute the possible stepsizes to calculate\n",
    "    alphas = np.linspace(0,1,iters)[::-1]\n",
    "    suff = False # Boolean to determine whether the stepsize was sufficient\n",
    "    for a in alphas:\n",
    "        fx = f(x)\n",
    "        fxap = f(x + a * p)\n",
    "        gradfx = gradf(x)\n",
    "        if fxap <= f(x) + c * a * gradf(x).dot(p):\n",
    "            suff = True\n",
    "            return suff, a\n",
    "    print \"Failed to converge\"\n",
    "    return suff, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.random.rand(500,11)\n",
    "y = (np.random.rand(500) > .5).astype(float)\n",
    "beta = np.random.rand(11)\n",
    "m = np.ones(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "526.45977804568952"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gradient_descent(X, y, beta, m, stepsize=1e-4, tol=1e-10, iters=1000):\n",
    "    '''\n",
    "    This function performs a simple gradient descent method to find the beta\n",
    "    that minimizes the negative log likelihood function.\n",
    "    \n",
    "    Inputs:\n",
    "    X: an m by n array of features\n",
    "    y: an m vector of successes\n",
    "    beta: an initial guess for the minimizer (for which we are solving)\n",
    "    m: an m vector of the number of trials\n",
    "    stepsize: a float, optional to specify the size of each gradient step\n",
    "    tol: a float, optional to specify the error size used to indicate convergence\n",
    "    iters; an integer, optional to specify the max number of iterations for the\n",
    "         algorithm\n",
    "         \n",
    "    Returns:\n",
    "    beta0: the minimizer of the negative log-likelihood\n",
    "    err: a list of the error between log-likelihoods for each time step\n",
    "    betas: a list of the new beta for each time step\n",
    "    ll: a list of the log likelihoods for each time step\n",
    "    '''\n",
    "    beta0 = beta\n",
    "    betas = [] # Stores the new value of beta at each step\n",
    "    ll = [] # Stores the new negative log likelihood at each step\n",
    "    for i in np.arange(iters):\n",
    "        betas.append(beta0)\n",
    "        ll.append(negloglike(X, y, beta0, m))\n",
    "        \n",
    "        beta1 = beta0 - stepsize * grad_lb(X, y, beta0, m)\n",
    "        \n",
    "        if np.abs(loglike(X,y,beta1,m) - loglike(X,y,beta0,m)) > tol:\n",
    "            beta0 = beta1            \n",
    "        else:\n",
    "            print i, \"iterations\" # Print the number of iterations.\n",
    "            return beta1, err, betas, ll \n",
    "    return beta0, betas, ll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quasi Newton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Problem A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem B"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
